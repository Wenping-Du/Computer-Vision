{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment3_task1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMOJD0_jdzzg",
        "colab_type": "text"
      },
      "source": [
        "# Task 1 Simple Perceptron training algorithm\n",
        "\n",
        "This code is written with numpy as the matrix manipulation module, a tutorial for which can be found [here](https://docs.scipy.org/doc/numpy/user/quickstart.html)\n",
        "\n",
        "You need the address the section of the code marked with **#TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sqkIpLjpVsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # This is for mathematical operations\n",
        "\n",
        "# this is used in plotting \n",
        "import matplotlib.pyplot as plt \n",
        "import time\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63hM2wTGugE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron:\n",
        "    \n",
        "    # input_size: dimension of the input including bias\n",
        "    def __init__(self,input_size):\n",
        "      \n",
        "        # we store the input size because we will need it later\n",
        "        self.input_size = input_size\n",
        "        \n",
        "        # weights (w) in randomly initalized to be the same size as the input\n",
        "        self.w = np.random.randn(input_size,1).reshape(input_size,1)\n",
        "        \n",
        "        # we will store our accuracy after each iteration here\n",
        "        self.history = []\n",
        "        \n",
        "    def train(self,X,Y, max_epochs = 100):\n",
        "      \n",
        "        # we clear history each time we start training\n",
        "        self.history = []\n",
        "        \n",
        "        converged = False\n",
        "        epochs = 0\n",
        "\n",
        "        while not converged and epochs < max_epochs :\n",
        "            \n",
        "            # TODO\n",
        "            # 1. add training code here that updates self.w \n",
        "            # 2.  a criteria to set converged to True under the correct circumstances. \n",
        "            \n",
        "            # curent strategy is random search (not good!)\n",
        "            self.w = np.random.randn(self.input_size,1)\n",
        "            \n",
        "            \n",
        "            # after training one epoch, we compute again the accuracy\n",
        "            self.compute_train_accuracy(X,Y)\n",
        "            epochs +=1\n",
        "        \n",
        "        if epochs == max_epochs:\n",
        "          print(\"Qutting: Reached max iterations\")\n",
        "          \n",
        "        if converged:\n",
        "          print(\"Qutting: Converged\")\n",
        "          \n",
        "        self.plot_training_history()\n",
        "    \n",
        "    # The draw function plots all the points and our current estimate \n",
        "    # of the boundary between the two classes. Point are colored according to\n",
        "    # the current output of the classifier. Ground truth boundary is also\n",
        "    # plotted since we know how we generated the data\n",
        "    \n",
        "    def draw(self,X):\n",
        "      \n",
        "        pl.close()\n",
        "        out = np.matmul(X,self.w).squeeze()\n",
        "        \n",
        "        P = X[out >= 0,:] \n",
        "        N = X[out.T < 0,:]\n",
        "        \n",
        "        x = np.linspace(0,1)\n",
        "        \n",
        "        pl.xlim((0,1))\n",
        "        pl.ylim((0,1))\n",
        " \n",
        "        pl.plot(P[:,0],P[:,1],'go', label = 'Positive')\n",
        "        pl.plot(N[:,0],N[:,1],'rx', label = 'Negative')\n",
        "        pl.plot(x, x, label = 'GT')\n",
        "        \n",
        "        a = self.w[0]\n",
        "        b = self.w[1]\n",
        "        c = self.w[2]\n",
        "        \n",
        "        pl.plot(x, -a/b * x - c/b, label = 'Estimated')\n",
        "        \n",
        "        pl.axis('tight')\n",
        "        pl.legend()\n",
        "        \n",
        "        display.clear_output(wait=True)\n",
        "        display.display(pl.gcf())\n",
        "        time.sleep(1)\n",
        "        \n",
        "    \n",
        "    # This computes the accuracy of our current estimate\n",
        "    def compute_train_accuracy(self,X,Y):\n",
        "        out = np.matmul(X,self.w)\n",
        "        Y_bar = (out >= 0)\n",
        "        accuracy = np.sum(Y==Y_bar)/np.float(Y_bar.shape[0])\n",
        "        self.history.append(accuracy)\n",
        "        print(\"Accuracy : %f \" % (accuracy))\n",
        "        self.draw(X)\n",
        "        \n",
        "    # Once training is done, we can plot the accuracy over time \n",
        "    def plot_training_history(self):\n",
        "      plt.ylim((0,1.01))\n",
        "      plt.plot(np.arange(len(self.history))+1, np.array(self.history),'-x')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.show()\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umh6ObbBuj3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_samples = 100\n",
        "max_number_of_epochs = 10\n",
        "\n",
        "X = np.random.rand(number_of_samples,2)\n",
        "X = np.append(X, np.ones((X.shape[0],1)),axis = 1)\n",
        "\n",
        "Y = X[:,1] > (X[:,0])\n",
        "Y = np.float32(Y)\n",
        "Y = Y.reshape((number_of_samples,1))\n",
        "\n",
        "p = Perceptron(3)\n",
        "p.train(X,Y,max_number_of_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ4MX2KVaFbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}