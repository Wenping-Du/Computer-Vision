{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment3_task3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOJjJbTit68G",
        "colab_type": "text"
      },
      "source": [
        "In this part of the assignment, we are going to implement an XORnet. \n",
        "\n",
        "![The XOR Truth Table](https://cdn-images-1.medium.com/max/1600/1*Xp1D32f-br8RCng5gjZMCA.gif)\n",
        "\n",
        "XOR is a problem that is not linearly separable, it means we need a hidden layer in our network. \n",
        "\n",
        "![XOR is not linearly separable](http://web.science.mq.edu.au/~cassidy/comp449/html/xor.gif)\n",
        "\n",
        "The network we are going implement below will use sigmoid function as activation and we are going to use Mean Squared Error as the loss. \n",
        "\n",
        "\n",
        "**Network design**\n",
        "\n",
        "The network has three layers:\n",
        "\n",
        "\n",
        "*  Input Layer (Taking two input A,B and a bias): This has a size of 3\n",
        "*  Hidden Layer (you can change the number of neuron in this layer)\n",
        "* Output Layer: This has a size of 1 as we are generating a single number for every input\n",
        "\n",
        "\n",
        "**Functions to implement **\n",
        "\n",
        "* forward(): That takes the current inputs, passes it through the network and generates and output y_hat\n",
        "* backward(): That does back-propagation to update weight. The sigmoid function and its derivative are provided. You have to derive the gradients **dW1** and **dW2** to updates weights *W1* and *W2*. The formula for update is given.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N69feef7r3m1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1+ np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
        "  \n",
        "class XORnet:\n",
        "    def __init__(self, x, y, h_size):\n",
        "      \n",
        "        # Inputs: \n",
        "        # x : the inputs\n",
        "        # y : the groundtruth outputs\n",
        "        # h_size : the number of nuerons in the hidden layer\n",
        "        \n",
        "        # we store x and y locally so we do not have to pass them everytime\n",
        "        self.input      = x\n",
        "        self.y          = y\n",
        "        \n",
        "        # W1 has a size of (3 x h_size)\n",
        "        self.W1   = np.random.rand(self.input.shape[1],h_size) \n",
        "        \n",
        "        #W2 has a size of (h_size x 1)\n",
        "        self.W2   = np.random.rand(h_size,1) \n",
        "        \n",
        "        \n",
        "        self.output     = np.zeros(self.y.shape) # This is y_hat (the output)\n",
        "\n",
        "    def forward(self):\n",
        "      \n",
        "        # TODO: \n",
        "        # implement the forward function that takes through each layer and\n",
        "        # the corresponding activation function, this will generate the \n",
        "        # output that should be stored in self.output\n",
        "\n",
        "        return np.dot((self.y - self.output).T, (self.y - self.output))\n",
        "      \n",
        "    def backward(self):\n",
        "        # TODO: \n",
        "        # apply the chain rule to find derivative of the loss function \n",
        "        # with respect to W2 and W1\n",
        "\n",
        "        d_W2 = 0 # Replace with correct derivative of the loss wrt W2\n",
        "        d_W1 = 0 # Replace with correct derivative of the loss wrt W1\n",
        "        \n",
        "        # we update the weights with the computed derivatives\n",
        "        # You do not need to edit this part\n",
        "        self.W2 +=  d_W2\n",
        "        self.W1 +=  d_W1\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1rITnNSpq08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_iterations = 1500\n",
        "num_hidden_neurons = 3\n",
        "\n",
        "# The four possible combination of two bits go as input,\n",
        "# along with a bias which is always set to 1\n",
        "# The input is organised as [A, B, bias] \n",
        "# Each row represents a sample\n",
        "\n",
        "X = np.array([[0,0,1],\n",
        "              [0,1,1],\n",
        "              [1,0,1],\n",
        "              [1,1,1.]]) \n",
        "\n",
        "# The corresponding output for each case\n",
        "\n",
        "Y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0.]])\n",
        "\n",
        "# This defines our XORnet and \n",
        "net = XORnet(X,Y,num_hidden_neurons)\n",
        "\n",
        "# We store losses after each epoch here \n",
        "losses = np.zeros((num_iterations,1))\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    loss = net.forward()\n",
        "    losses[i] = loss\n",
        "    net.backward()\n",
        "\n",
        "print(\"Expected Output: \\n\",  (Y.T))\n",
        "print(\"Current output :\\n\", net.output.T)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r-lWLBTr8CQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}